{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project I: Parameter estimation for a toy model of an effective field theory\n",
    "## Learning from data [TIF285], Chalmers, Fall 2019\n",
    "\n",
    "Last revised: 12-Sep-2019 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See deadline on the course web page\n",
    "- This project is performed in groups of two students. \n",
    "- The second part of the project is optional and only gives extra points. See examination rules on the course web page.\n",
    "- Hand-in your written report via Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written report\n",
    "- Page limit: 6 pages (excluding title page and list of references). 3 extra pages are allowed when doing also the optional extra task.\n",
    "- Give a short description of the nature of the problem and the methods you have used.\n",
    "- Include your results either in figure form or in a table. All tables and figures should have relevant captions and labels on the axes.\n",
    "- Try to give an interpretation of you results.\n",
    "- Upload the source code of your program as a separate file (.ipynb or .py). Comment your program properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main task\n",
    "The overall project goal is to reproduce various results in a paper: [*Bayesian parameter estimation for effective field theories*](https://arxiv.org/abs/1511.03618).  It's a long paper, so don't try to read all of it!  We'll guide you to the relevant parts. Sec. II of the paper is the most relevant, but you don't need to understand all of it. \n",
    "\n",
    "The paper uses toy models for effective field theories (EFTs), namely Taylor series of some specified functions, to present guidelines for parameter estimation. We will not discuss the EFT concept in any detail, but will just mention that it is a type of approximation, or effective theory, for an underlying physical theory, such as a quantum field theory. An EFT includes the appropriate degrees of freedom to describe physical phenomena occurring at a chosen length scale or energy scale, while not resolving substructure and degrees of freedom at shorter distances (or, equivalently, at higher energies). \n",
    "\n",
    "In our case, the function\n",
    "$$\n",
    "g(x) = \\left(\\frac12 + \\tan\\left(\\frac{\\pi}{2}x\\right)\\right)^2\n",
    "$$\n",
    "represents the true, underlying theory.  It has a Taylor expansion\n",
    "$$\n",
    "g(x) = 0.25 + 1.57 x + 2.47 x^2 + 1.29 x^3 + \\cdots\n",
    "$$\n",
    "that is known for this toy example (but would not be in a real situation).\n",
    "\n",
    "Our model for an EFT for this \"theory\" is then\n",
    "$$\n",
    "g_{\\rm th}(x) \\equiv \\sum_{i=0}^k a_i x^i,\n",
    "$$\n",
    "where $a_i$ are the parameters of the EFT, sometimes known as low-energy constants (LECs).\n",
    "\n",
    "Your general task is to fit a few of the parameters $a_i$ using a Bayesian approach, and analyze the results. \n",
    "\n",
    "* **Your specific goal is to reproduce and interpret Figure 1 on page 6 of the arXiv preprint. This figure shows joint pdfs for the EFT parameters using two different priors. One of these priors encapsulates the physics expectation that the parameters should be of \"natural\" size (order one).**\n",
    "* **A secondary goal is to reproduce Figs. 3 and 4 that show predictions of the inferred EFT with error bands (you can use $k=k_\\mathrm{max}=3$ as in the first task).**. \n",
    "\n",
    "You should use emcee to sample the joint pdfs and you should use corner to make plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning goals:\n",
    "* Apply and extend the Bayesian parameter estimation ideas and techniques from the course.\n",
    "* This a less-guided set of tasks and you will have to put together ideas and tools we've discussed.\n",
    "* Explore the impact of control features:  dependence on how much data is used and how precise it is; apply an *informative* prior.\n",
    "* Learn about some diagnostics for Bayesian parameter estimation.\n",
    "* Try out sampling on a controlled problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for how to proceed:\n",
    "* Follow the lead of the exercise notebooks.\n",
    "* Load the data set that was used in the paper: [D1_c_5.dat](https://arxiv.org/src/1511.03618v3/anc/D1_c_5.dat). The three columns correspond respectively to $x_j$, the measured data $d(x_j) \\equiv d_j$ and the error in terms of a standard deviation $\\sigma_j$.\n",
    "* Define functions for the two choices of prior: a uniform prior, $|a_i|<100$, and a Gaussian naturalness prior given by Eq. (24) with $\\bar{a}=5$. Use the log prior.\n",
    "* Define a function for the likelihood (Eq. 9). It will require the chi-squared measure. Use the log likelihood.\n",
    "* Call emcee to sample the posteriors. \n",
    "* Use corner to create plots.  You can read the answers for the tables from the corner plots.\n",
    "* Don't try to do too much in your code at first (start with a low order). \n",
    "* Generate figures for the lowest orders analogous to Figure 1 and then reproduce Figure 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments and suggestions\n",
    "* The `show_titles=True` option to corner will show central results and one-$\\sigma$ error limits on the projected posterior plots.\n",
    "* The `quantiles=[0.16, 0.5, 0.84]`option to corner adds the dashed vertical lines to the marginal posteriors on the diagonal. You can obviously change the quantiles if you want another credibility region.\n",
    "* The python command `np.percentile(samples, [16, 50, 84],axis=0)` might be useful to extract numerical values for the credibility region and the mean from a python array `samples`of shape (nsamples,ndimensions).\n",
    "* When reproducing Figures 3 and 4 you can use Matplotlib's `fill_between(x, y-error, y+error)` to make bands.  (Use the `alpha` keyword for `fill_between`, e.g., `alpha=0.5`, to make the bands more transparent.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Part 2 (extra points)\n",
    "* Reproduce and interpret Table III on page 12 of the arXiv preprint. \n",
    "* Repeat analysis with same function but different data precision and/or quantity (number of data points). You will then have to generate the data yourself using Eq. (2) and the true function Eq. (23). It is probably wise to stay in the $0 < x \\le 1/\\pi$ range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
