{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Learning from data: Linear Regression -->\n",
    "# Learning from data: Linear Regression\n",
    "<!-- dom:AUTHOR: Christian Forssén at Department of Physics, Chalmers University of Technology, Sweden -->\n",
    "<!-- Author: -->  \n",
    "**Christian Forssén**, Department of Physics, Chalmers University of Technology, Sweden  \n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University -->\n",
    "<!-- Author: --> **Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **Aug 30, 2020**\n",
    "\n",
    "Copyright 2018-2020, Christian Forssén. Released under CC Attribution-NonCommercial 4.0 license\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Linear regression\n",
    "\n",
    "## Why Linear Regression (aka Ordinary Least Squares)\n",
    "\n",
    "Fitting a continuous function with linear parameterization in terms of the parameters  $\\boldsymbol{\\theta}$.\n",
    "* Often used for fitting a continuous function!\n",
    "\n",
    "* Gives an excellent introduction to central Machine Learning features with **understandable pedagogical** links to other methods like **Neural Networks**, **Support Vector Machines** etc\n",
    "\n",
    "* Analytical expression for the fitting parameters $\\boldsymbol{\\theta}$\n",
    "\n",
    "* Analytical expressions for statistical propertiers like mean values, variances, confidence intervals and more\n",
    "\n",
    "* Analytical relation with probabilistic interpretations \n",
    "\n",
    "* Easy to introduce basic concepts like bias-variance tradeoff, cross-validation, resampling and regularization techniques and many other ML topics\n",
    "\n",
    "* Easy to code! And links well with classification problems and logistic regression and neural networks\n",
    "\n",
    "* Allows for **easy** hands-on understanding of gradient descent methods\n",
    "\n",
    "### Regression analysis, overarching aims\n",
    "\n",
    "\n",
    "Regression modeling deals with the description of  the sampling distribution of a given random variable $y$ and how it varies as function of another variable or a set of such variables $\\boldsymbol{x} =[x_0, x_1,\\dots, x_{n-1}]^T$. \n",
    "The first variable is called the **dependent**, the **outcome** or the **response** variable while the set of variables $\\boldsymbol{x}$ is called the **independent** variable, or the **predictor** variable or the **explanatory** variable. \n",
    "\n",
    "A regression model $M$ aims at finding a likelihood function $p(\\boldsymbol{y}\\vert \\boldsymbol{x},M,\\mathcal{D}_n)$, that is the conditional distribution for $\\boldsymbol{y}$ given the independent variable $\\boldsymbol{x}$ and a model $M$ that has been trained on a data set $\\mathcal{D}_n$. The data set consists of: \n",
    "* $n$ cases $i = 0, 1, 2, \\dots, n-1$ \n",
    "\n",
    "* Response variable $y_i$ with $i = 0, 1, 2, \\dots, n-1$. These are sometimes referred to as target, dependent or outcome.\n",
    "\n",
    "* For each case there will be $p$ so-called explanatory (independent or predictor) variables $\\boldsymbol{x}_i=[x_{i0}, x_{i1}, \\dots, x_{ip-1}]$ with $i = 0, 1, 2, \\dots, n-1$ and explanatory variables running from $0$ to $p-1$. See below for more explicit examples. \n",
    "\n",
    " The goal of the regression analysis is to extract/exploit a relationship between $\\boldsymbol{y}$ and $\\boldsymbol{x}$ or to infer causal dependencies, and to make fits, and predictions, and many other things.\n",
    "\n",
    "\n",
    "\n",
    "The $p$ explanatory variables for the $n$ cases in the data set are normally represented by a matrix $\\mathbf{X}$.\n",
    "\n",
    "The matrix $\\mathbf{X}$ is called the *design\n",
    "matrix*. In addition, each case is also represented by its *response variable* $\\boldsymbol{y}$. The aim of\n",
    "regression analysis is to explain $\\boldsymbol{y}$ in terms of\n",
    "$\\boldsymbol{X}$ through a functional relationship like $y_i =\n",
    "f(\\mathbf{X}_{i},\\ast )$.\n",
    "\n",
    "It is common to assume a linear relationship\n",
    "between $\\boldsymbol{X}$ and $\\boldsymbol{y}$. This assumption gives rise to\n",
    "the *linear regression model* where $\\boldsymbol{\\theta} = [\\theta_0, \\ldots,\n",
    "\\theta_{p-1}]^{T}$ are the *regression parameters*. Linear regression gives us a set of analytical equations for the parameters $\\theta_j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Example: Liquid-drop model for nuclear binding energies\n",
    "\n",
    "In order to understand the relation among the predictors $p$, the set of data $\\mathcal{D}_n$ and the target (outcome, output etc) $\\boldsymbol{y}$,\n",
    "consider the model we discussed for describing nuclear binding energies. \n",
    "\n",
    "There we assumed that we could parametrize the data using a polynomial approximation based on the liquid drop model.\n",
    "Assuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "BE(A,N,Z) = a_0+a_1A+a_2A^{2/3}+a_3 Z^2 A^{-1/3}+a_4 (N-Z)^2 A^{-1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have five predictors, that is the intercept (constant term, aka bias), the $A$ dependent term, the $A^{2/3}$ term and the $Z^2 A^{-1/3}$ and $(N-Z)^2 A^{-1}$ terms. Although the predictors are somewhat complicated functions of $A,N,Z$, we note that the $p=5$ regression parameters $\\theta = (a_0, a_1, a_2, a_3, a_4)$ enter linearly. Furthermore we have $n$ cases. It means that our design matrix is a \n",
    "$p\\times n$ matrix $\\boldsymbol{X}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Polynomial basis functions\n",
    "Let us study a case from linear algebra where we aim at fitting a set of data $\\boldsymbol{y}=[y_0,y_1,\\dots,y_{n-1}]$. We could think of these data as a result of an experiment or a complicated numerical experiment. These data are functions of a variable $x$ so that for the data set we have $\\boldsymbol{x}=[x_0,x_1,\\dots,x_{n-1}]$ and $y_i = y(x_i)$ with $i=0,1,2,\\dots,n-1$. The variable $x_i$ could represent a physical quantity like time, temperature, position etc. We assume that $y(x)$ is a smooth function. \n",
    "\n",
    "Now, we don't know $y(x)$ but we want to use the data that we have to fit a function which can allow us to make predictions for values of $y$ which are not in the present set. The perhaps simplest approach is to assume we can parametrize our function in terms of a polynomial $f(x)$ of degree $p-1$. Since we realize that our polynomial model might not represent $y(x)$ perfectly we also add an error term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y=y(x) \\rightarrow y(x_i)=f({x}_i)+\\epsilon_i=\\sum_{j=0}^{p-1} \\theta_j x_i^j+\\epsilon_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\epsilon_i$ is the error in our approximation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For every set of values $y_i,x_i$ we have thus the corresponding set of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "y_0&=\\theta_0+\\theta_1x_0^1+\\theta_2x_0^2+\\dots+\\theta_{p-1}x_0^{p-1}+\\epsilon_0\\\\\n",
    "y_1&=\\theta_0+\\theta_1x_1^1+\\theta_2x_1^2+\\dots+\\theta_{p-1}x_1^{p-1}+\\epsilon_1\\\\\n",
    "y_2&=\\theta_0+\\theta_1x_2^1+\\theta_2x_2^2+\\dots+\\theta_{p-1}x_2^{p-1}+\\epsilon_2\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{n-1}&=\\theta_0+\\theta_1x_{n-1}^1+\\theta_2x_{n-1}^2+\\dots+\\theta_{p-1}x_{n-1}^{p-1}+\\epsilon_{n-1}.\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y} = [y_0,y_1, y_2,\\dots, y_{n-1}]^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta} = [\\theta_0,\\theta_1, \\theta_2,\\dots, \\theta_{p-1}]^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\epsilon} = [\\epsilon_0,\\epsilon_1, \\epsilon_2,\\dots, \\epsilon_{n-1}]^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the design matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\n",
    "\\begin{bmatrix} \n",
    "1& x_{0}^1 &x_{0}^2& \\dots & \\dots &x_{0}^{p-1}\\\\\n",
    "1& x_{1}^1 &x_{1}^2& \\dots & \\dots &x_{1}^{p-1}\\\\\n",
    "1& x_{2}^1 &x_{2}^2& \\dots & \\dots &x_{2}^{p-1}\\\\                      \n",
    "\\dots& \\dots &\\dots& \\dots & \\dots &\\dots\\\\\n",
    "1& x_{n-1}^1 &x_{n-1}^2& \\dots & \\dots &x_{n-1}^{p-1}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can rewrite our equations as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above design matrix is called a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### General basis functions\n",
    "\n",
    "\n",
    "We are obviously not limited to the above polynomial expansions.  We\n",
    "could replace the various powers of $x$ with elements of Fourier\n",
    "series or instead of $x_i^j$ we could have $\\cos{(j x_i)}$ or $\\sin{(j\n",
    "x_i)}$, or time series or other orthogonal functions.  For every set\n",
    "of values $y_i,x_i$ we can then generalize the equations to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "y_0&=\\theta_0x_{00}+\\theta_1x_{01}+\\theta_2x_{02}+\\dots+\\theta_{p-1}x_{0p-1}+\\epsilon_0\\\\\n",
    "y_1&=\\theta_0x_{10}+\\theta_1x_{11}+\\theta_2x_{12}+\\dots+\\theta_{p-1}x_{1p-1}+\\epsilon_1\\\\\n",
    "y_2&=\\theta_0x_{20}+\\theta_1x_{21}+\\theta_2x_{22}+\\dots+\\theta_{p-1}x_{2p-1}+\\epsilon_2\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{i}&=\\theta_0x_{i0}+\\theta_1x_{i1}+\\theta_2x_{i2}+\\dots+\\theta_{p-1}x_{ip-1}+\\epsilon_i\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{n-1}&=\\theta_0x_{n-1,0}+\\theta_1x_{n-1,2}+\\theta_2x_{n-1,2}+\\dots+\\theta_{p-1}x_{n-1,p-1}+\\epsilon_{n-1}.\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine in turn the matrix $\\boldsymbol{X}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\n",
    "\\begin{bmatrix} \n",
    "x_{00}& x_{01} &x_{02}& \\dots & \\dots &x_{0,p-1}\\\\\n",
    "x_{10}& x_{11} &x_{12}& \\dots & \\dots &x_{1,p-1}\\\\\n",
    "x_{20}& x_{21} &x_{22}& \\dots & \\dots &x_{2,p-1}\\\\                      \n",
    "\\dots& \\dots &\\dots& \\dots & \\dots &\\dots\\\\\n",
    "x_{n-1,0}& x_{n-1,1} &x_{n-1,2}& \\dots & \\dots &x_{n-1,p-1}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and without loss of generality we rewrite again our equations as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left-hand side of this equation is kwown. The error vector $\\boldsymbol{\\epsilon}$ and the parameter vector $\\boldsymbol{\\theta}$ are unknown quantities. How can we obtain the optimal set of $\\theta_i$ values?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have defined the matrix $\\boldsymbol{X}$ via the equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "y_0&=\\theta_0x_{00}+\\theta_1x_{01}+\\theta_2x_{02}+\\dots+\\theta_{p-1}x_{0p-1}+\\epsilon_0\\\\\n",
    "y_1&=\\theta_0x_{10}+\\theta_1x_{11}+\\theta_2x_{12}+\\dots+\\theta_{p-1}x_{1p-1}+\\epsilon_1\\\\\n",
    "y_2&=\\theta_0x_{20}+\\theta_1x_{21}+\\theta_2x_{22}+\\dots+\\theta_{p-1}x_{2p-1}+\\epsilon_1\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{i}&=\\theta_0x_{i0}+\\theta_1x_{i1}+\\theta_2x_{i2}+\\dots+\\theta_{p-1}x_{ip-1}+\\epsilon_1\\\\\n",
    "\\dots & \\dots \\\\\n",
    "y_{n-1}&=\\theta_0x_{n-1,0}+\\theta_1x_{n-1,2}+\\theta_2x_{n-1,2}+\\dots+\\theta_{p-1}x_{n-1,p-1}+\\epsilon_{n-1}.\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the design matrix \n",
    " $\\boldsymbol{X}\\in {\\mathbb{R}}^{n\\times p}$, with the predictors refering to the column numbers and the entries $n$ being the row elements.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "With the above we use the design matrix to define the approximation $\\boldsymbol{\\tilde{y}}$ via the unknown quantity $\\boldsymbol{\\theta}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\tilde{y}}= \\boldsymbol{X}\\boldsymbol{\\theta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in order to find the optimal parameters $\\theta_i$ instead of solving the above linear algebra problem, we define a function which gives a measure of the spread between the values $y_i$ (which represent hopefully the exact values) and the parameterized values $\\tilde{y}_i$, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}}\\right)\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or using the matrix $\\boldsymbol{X}$ and in a more compact matrix-vector notation as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta})=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is one possible way to define the so-called **cost function**.\n",
    "\n",
    "\n",
    "\n",
    "It is also common to define\n",
    "the cost function as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta})=\\frac{1}{2n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since when taking the first derivative with respect to the unknown parameters $\\theta$, the factor of $2$ cancels out.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta})=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be linked to the variance of the quantity $y_i$ if we interpret the latter as the mean value. \n",
    "When linking (see the discussion below) with the maximum likelihood approach, we will indeed interpret $y_i$ as a mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_{i}=\\langle y_i \\rangle = \\theta_0x_{i,0}+\\theta_1x_{i,1}+\\theta_2x_{i,2}+\\dots+\\theta_{n-1}x_{i,n-1}+\\epsilon_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\langle y_i \\rangle$ is the mean value. Keep in mind also that\n",
    "till now we have treated $y_i$ as the exact value. Normally, the\n",
    "response (dependent or outcome) variable $y_i$ the outcome of a\n",
    "numerical experiment or another type of experiment and is thus only an\n",
    "approximation to the true value. It is then always accompanied by an\n",
    "error estimate, often limited to a statistical error estimate given by\n",
    "the standard deviation discussed earlier. In the discussion here we\n",
    "will treat $y_i$ as our exact value for the response variable.\n",
    "\n",
    "In order to find the parameters $\\theta_i$ we will then minimize the spread of $C(\\boldsymbol{\\theta})$, that is we are going to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical terms it means we will require"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{\\theta})}{\\partial \\theta_j} = \\frac{\\partial }{\\partial \\theta_j}\\left[ \\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\theta_0x_{i,0}-\\theta_1x_{i,1}-\\theta_2x_{i,2}-\\dots-\\theta_{n-1}x_{i,n-1}\\right)^2\\right]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which results in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{\\theta})}{\\partial \\theta_j} = -\\frac{2}{n}\\left[ \\sum_{i=0}^{n-1}x_{ij}\\left(y_i-\\theta_0x_{i,0}-\\theta_1x_{i,1}-\\theta_2x_{i,2}-\\dots-\\theta_{n-1}x_{i,n-1}\\right)\\right]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in a matrix-vector form as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = 0 = \\boldsymbol{X}^T\\left( \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = 0 = \\boldsymbol{X}^T\\left( \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{y} = \\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$ is invertible we have the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta} =\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note also that since our design matrix is defined as $\\boldsymbol{X}\\in\n",
    "{\\mathbb{R}}^{n\\times p}$, the product $\\boldsymbol{X}^T\\boldsymbol{X} \\in\n",
    "{\\mathbb{R}}^{p\\times p}$.  In the liquid drop model example from the Intro lecture, we had $p=5$ ($p \\ll n$) meaning that we end up with inverting a small\n",
    "$5\\times 5$ matrix. This is a rather common situation, in many cases we end up with low-dimensional\n",
    "matrices to invert, which\n",
    "allow for the usage of direct linear algebra methods such as **LU** decomposition or **Singular Value Decomposition** (SVD) for finding the inverse of the matrix\n",
    "$\\boldsymbol{X}^T\\boldsymbol{X}$.\n",
    "\n",
    "\n",
    "\n",
    "**Small question**: What kind of problems can we expect when inverting the matrix  $\\boldsymbol{X}^T\\boldsymbol{X}$?\n",
    "\n",
    "\n",
    "\n",
    "## Training scores\n",
    "\n",
    "We can easily test our fit by computing various **training scores**. Several such measures are used in machine learning applications. First we have the **Mean-Squared Error** (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_{\\mathrm{data},i} - y_{\\mathrm{model},i}(\\boldsymbol{\\theta}) \\right)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have $n$ training data and our model is a function of the parameter vector $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Furthermore, we have the **mean absolute error** (MAE) defined as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{MAE}(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\left| y_{\\mathrm{data},i} - y_{\\mathrm{model},i}(\\boldsymbol{\\theta}) \\right|,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the $R2$ score, also known as *coefficient of determination* is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{R2}(\\boldsymbol{\\theta}) = 1 - \\frac{\\sum_{i=1}^n \\left( y_{\\mathrm{data},i} - y_{\\mathrm{model},i}(\\boldsymbol{\\theta}) \\right)^2}{\\sum_{i=1}^n \\left( y_{\\mathrm{data},i} - \\bar{y}_\\mathrm{model}(\\boldsymbol{\\theta}) \\right)^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\bar{y}_\\mathrm{model}(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n y_{\\mathrm{model},i} (\\boldsymbol{\\theta})$ is the mean of the model predictions.\n",
    "\n",
    "\n",
    "### The $\\chi^2$ function\n",
    "\n",
    "\n",
    "Normally, the response (dependent or outcome) variable $y_i$ is the\n",
    "outcome of a numerical experiment or another type of experiment and is\n",
    "thus only an approximation to the true value. It is then always\n",
    "accompanied by an error estimate, often limited to a statistical error\n",
    "estimate given by the standard deviation discussed earlier. \n",
    "\n",
    "Introducing the standard deviation $\\sigma_i$ for each measurement\n",
    "$y_i$ (assuming uncorrelated errors), we define the $\\chi^2$ function as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\chi^2(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=0}^{n-1}\\frac{\\left(y_i-\\tilde{y}_i\\right)^2}{\\sigma_i^2}=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}}\\right)^T \\boldsymbol{\\Sigma}^{-1}\\left(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}}\\right)\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the matrix $\\boldsymbol{\\Sigma}$ is a diagonal $n \\times n$ matrix with $\\sigma_i^2$ as matrix elements.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In order to find the parameters $\\theta_i$ we will then minimize the spread of $\\chi^2(\\boldsymbol{\\theta})$ by requiring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\boldsymbol{\\theta})}{\\partial \\theta_j} = \\frac{\\partial }{\\partial \\theta_j}\\left[ \\frac{1}{n}\\sum_{i=0}^{n-1}\\left(\\frac{y_i-\\theta_0x_{i,0}-\\theta_1x_{i,1}-\\theta_2x_{i,2}-\\dots-\\theta_{n-1}x_{i,n-1}}{\\sigma_i}\\right)^2\\right]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which results in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\boldsymbol{\\theta})}{\\partial \\theta_j} = -\\frac{2}{n}\\left[ \\sum_{i=0}^{n-1}\\frac{x_{ij}}{\\sigma_i}\\left(\\frac{y_i-\\theta_0x_{i,0}-\\theta_1x_{i,1}-\\theta_2x_{i,2}-\\dots-\\theta_{n-1}x_{i,n-1}}{\\sigma_i}\\right)\\right]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in a matrix-vector form as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = 0 = \\boldsymbol{A}^T\\left( \\boldsymbol{b}-\\boldsymbol{A}\\boldsymbol{\\theta}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have defined the matrix $\\boldsymbol{A} =\\boldsymbol{X} \\boldsymbol{\\Sigma}^{-1/2}$ with matrix elements $a_{ij} = x_{ij}/\\sigma_i$ and the vector $\\boldsymbol{b}$ with elements $b_i = y_i/\\sigma_i$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\chi^2(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = 0 = \\boldsymbol{A}^T\\left( \\boldsymbol{b}-\\boldsymbol{A}\\boldsymbol{\\theta}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{A}^T\\boldsymbol{b} = \\boldsymbol{A}^T\\boldsymbol{A}\\boldsymbol{\\theta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if the matrix $\\boldsymbol{A}^T\\boldsymbol{A}$ is invertible we have the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta} =\\left(\\boldsymbol{A}^T\\boldsymbol{A}\\right)^{-1}\\boldsymbol{A}^T\\boldsymbol{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then introduce the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{H} =  \\left(\\boldsymbol{A}^T\\boldsymbol{A}\\right)^{-1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have then the following expression for the parameters $\\theta_j$ (the matrix elements of $\\boldsymbol{H}$ are $h_{ij}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_j = \\sum_{k=0}^{p-1}h_{jk}\\sum_{i=0}^{n-1}\\frac{y_i}{\\sigma_i}\\frac{x_{ik}}{\\sigma_i} = \\sum_{k=0}^{p-1}h_{jk}\\sum_{i=0}^{n-1}b_ia_{ik}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We state without proof the expression for the uncertainty  in the parameters $\\theta_j$ as (we leave this as an exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2(\\theta_j) = \\sum_{i=0}^{n-1}\\sigma_i^2\\left( \\frac{\\partial \\theta_j}{\\partial y_i}\\right)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resulting in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2(\\theta_j) = \\left(\\sum_{k=0}^{p-1}h_{jk}\\sum_{i=0}^{n-1}a_{ik}\\right)\\left(\\sum_{l=0}^{p-1}h_{jl}\\sum_{m=0}^{n-1}a_{ml}\\right) = h_{jj}!\n",
    "$$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
