{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- See deadline on the course web page\n",
    "- This problem set is solved individually. See examination rules on the course web page and the explanation of the examination procedure below.\n",
    "- The two notebooks for each problem set contain a number of basic and extra problems; you can choose which and how many to work on. The extra problems are usually more challenging.\n",
    "- Students are allowed to discuss together and help each other when solving the problems. However, every student must understand their submitted solution in the sense that they should be able to explain and discuss them with a peer or with a teacher.\n",
    "- While discussions with your peers are allowed (and even encouraged), direct plagiarism is not. Every student must reach their own understanding of submitted solutions according to the definition in the previous point.\n",
    "- The use of coding assistance from code generating artificial intelligence tools is allowed. However, every student must reach their own understanding of submitted solutions (including employed algorithms) according to the definition above.\n",
    "- Some problems include checkpoints in the form of `assert` statements. These usually check some basic functionality and you should make sure that your code passes these statements without raising an `AssertionError`. \n",
    "- Do not use other python modules than the ones included in the `environment.yml` file in the course github repo. \n",
    "\n",
    "- **Important:** The grading of problem sets requires **all** of the following actions:\n",
    "  1. Make sure to always complete **Task 0** in the header part of the notebook and that this part does not raise any `AssertionError`(s).\n",
    "  1. **Complete** the corresponding questions in Yata for every task that you have completed. This usually involves copying and pasting some code from your solution notebook and passing the code tests. You need to have a green check mark on Yata to get the corresponding points.\n",
    "  1. **Upload** your solution in the form of your edited version of this Jupyter notebook via the appropriate assignment module in Canvas (separate for basic and extra tasks). It is the code and results in your submitted notebook that is considered to be your hand-in solution.\n",
    "  1. If selected, be **available for a discussion** of your solution with one of the teachers on the Monday afternoon exercise session directly following the problem set deadline. No extra preparation is needed for these discussions apart from familiarity with your own solution. A list of randomly selected students will be published on the course web page around Monday noon. During the afternoon session that same day, students will be called in the numbered order until the end of the list (or the end of the exercise session). You must inform the responsible teacher as soon as possible following the publication of the student list if you can not be physically present at the exercise session (in which case we will have the discussion on zoom). An oral examination (on all aspects of the course) will be arranged during the exam week for students that do not show up for their discussion slot, or that fail to demonstrate familiarity with their hand-in solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "- Make sure that the **run time is smaller than a few minutes**. If needed you might have to reduce some computational tasks; e.g. by decreasing the number of grid points or sampling steps. Please ask the supervisors if you are uncertain about the run time. \n",
    "\n",
    "- Your solutions are usually expected where it says `YOUR CODE HERE` or <font color=\"red\">\"PLEASE WRITE YOUR ANSWER HERE\"</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 \n",
    "#### (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "251105c400969a6d24e9f7ec4883888e",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "- All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64a7020fd0e62e5b51ef4470ae1c797f",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student_self_assessment = False\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 (Basic problems)\n",
    "**Learning from data [TIF285], Chalmers, Fall 2023**\n",
    "\n",
    "Last revised: 11-Sep-2023 by Christian ForssÃ©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c532e9c42f0ac2234f7eb5f88c73672b",
     "grade": false,
     "grade_id": "cell-f3768fbcb502fd03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Data files are stored in\n",
    "DATA_DIR = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Coin tossing (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata):** Define a function `bayesian_analysis_coin_flips` (see start code below) that returns the mean, median, and 68%/95% credible intervals of the Bayesian posterior with an input data array of coin flips and using a uniform $[0,1]$ prior for `pH` (probability of heads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (mainly for testing your code; not on Yata)** Read the data with simulated coin tosses from the file `cointosses.dat`.\n",
    "Each row corresponds to a single toss: 0=tails; 1=heads\n",
    "\n",
    "Extract the mean and the 95% credible intervals (Degree-of-belief or DoB intervals) from the first 8 tosses, the first 64 tosses, the first 512 tosses and all 4096 tosses in the data assuming a uniform prior for the probability $p_H$ of obtaining heads in a single toss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint*: Sample code for computing the DoB interval is available in the demonstration notebook `demo-BayesianBasics.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f5396eeb530ca64889f4e7444703265",
     "grade": false,
     "grade_id": "cell-f27c4016e4570c15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#...\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd34410ae6a9662eac24de21527e4929",
     "grade": false,
     "grade_id": "cell-71d9ceb95f1f80db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = np.loadtxt(f'{DATA_DIR}/PS2_Prob1_cointosses.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cc3e4665a66c30f9b147ce475a5973e",
     "grade": false,
     "grade_id": "cell-79bec52ebb8e7d12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional. \n",
    "# Insert utility code / functions here.\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5c76c6a0fb8a7e204839fbc79d600aa",
     "grade": false,
     "grade_id": "cell-538f840bea75b7dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that returns the mean, median, and 68%/95% credible intervals \n",
    "# of the Bayesian posterior with an input data array of coin flips \n",
    "# and using a uniform [0,1] prior for the pH (probability of heads).\n",
    "\n",
    "def bayesian_analysis_coin_flips(data_coin_tosses):\n",
    "    \"\"\"\n",
    "    Returns various Bayesian analysis results for the given data of coin tosses.\n",
    "    \n",
    "    The posterior is p( pH | data, I).\n",
    "    Assume a uniform p(pH|I) = U[0,1] prior\n",
    "    \n",
    "    Args:\n",
    "        data_coin_tosses: Array of shape (m,) with 'm' independent binary data.\n",
    "            0 = tails; 1 = heads\n",
    "            \n",
    "    Returns:\n",
    "        (mean, mode, median, dob68, dob95): A tuple with the following elements\n",
    "            mean: The mean of the posterior distribution (float)\n",
    "            mode: The mode of the posterior distribution (float)\n",
    "            median: The median of the posterior distribution (float)\n",
    "            dob68: A tuple (lo,hi) with the lower and upper limits of the \n",
    "                68% degree-of-belief range of the posterior distribution (float,float)\n",
    "            dob95: A tuple (lo,hi) with the lower and upper limits of the \n",
    "                95% degree-of-belief range of the posterior distribution (float,float)\n",
    "    \"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b554d6660047c1cfc909c52ac6357d5",
     "grade": true,
     "grade_id": "cell-a567356cf18d3da5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (mean, mode, median, dob68, dob95) \u001b[38;5;241m=\u001b[39m bayesian_analysis_coin_flips(data[:\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m (mean, mode, median, dob68[\u001b[38;5;241m0\u001b[39m], dob95[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m output\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWrong type\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:1])\n",
    "for output in (mean, mode, median, dob68[0], dob95[0]):\n",
    "    assert output.dtype=='float64', 'Wrong type'\n",
    "assert len(dob68)==2, 'DoB tuple should be of length 2'\n",
    "assert len(dob95)==2, 'DoB tuple should be of length 2'\n",
    "assert np.abs(mean-0.667)<0.001\n",
    "assert np.abs(mode-1.0)<0.001\n",
    "assert np.abs(median-0.707)<0.001\n",
    "\n",
    "# 8 tosses\n",
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:8])\n",
    "assert np.abs(mode-0.3750)<0.001\n",
    "assert np.abs(dob68[0]-0.2469)<0.001\n",
    "assert np.abs(dob68[1]-0.5538)<0.001\n",
    "# 1024 tosses\n",
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:1024])\n",
    "assert np.abs(median-0.4922)<0.001\n",
    "# 4096 tosses\n",
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:4096])\n",
    "assert np.abs(dob95[0]-0.4727)<0.001\n",
    "assert np.abs(dob95[1]-0.5033)<0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Bayesian linear regression (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fitting a linear model (in this case a first order polynomial) to a set data. Our model has two parameters $\\vec\\theta=[\\theta_0,\\theta_1]$ (pay attention to the indexing)\n",
    "\n",
    "$$\n",
    "y_M(x) = \\theta_0 + \\theta_1 x \n",
    "$$\n",
    "\n",
    "And our statistical model assumes that errors are independent and identically distributed\n",
    "\n",
    "$$\n",
    "y_i = y_M(x_i;\\theta) + \\varepsilon_i.\n",
    "$$\n",
    "\n",
    "Specifically, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ and we assume a fixed standard deviation $\\sigma = 40$. \n",
    "\n",
    "(Note that the $\\varepsilon_i \\sim \\ldots$ notation means that $\\varepsilon_i$ is a draw of a random variable that follows the specified distribution.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is generated from a straight line with intercept = 15. and slope = 1.5 plus random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 15.\n",
    "slope = 1.5\n",
    "theta_true = np.array([intercept, slope])\n",
    "sigma=40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the file `PS2_Prob2_data.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dd517c45e4d41c20b1926e35aec8596",
     "grade": false,
     "grade_id": "cell-0609945db7cac2cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data and plot with fixed error bar \n",
    "# Use np.loadtxt() for loading data (the argument 'unpack=True' is useful)\n",
    "# and plt.errorbar() for plotting data with errorbars\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Bayesian linear regression: Implement log prior(s) and likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata)** Create functions that return the natural logarithm of the likelihood and two different choices of prior. Since we are seeking a parameter posterior you do not have to use proper normalization of the probability densities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two different choices for the prior:\n",
    "1. A uniform prior:\n",
    "   $$\n",
    "   p(\\theta_0, \\theta_1 | I) \\propto \\left\\{ \\begin{array}{ll} 1 & \\text{if } -100 \\le \\theta_0 \\le 100 \\text{ and } -100 \\le \\theta_1 \\le 100 \\\\ 0 & \\text{else} \\end{array}\\right.\n",
    "   $$\n",
    "2. A uniform prior for the intercept and a symmetry-invariant one for the slope\n",
    "   $$\n",
    "   p(\\theta_0, \\theta_1 | I) \\propto \\left\\{ \\begin{array}{ll} \\frac{1}{(1+\\theta_1^2)^{3/2}} & \\text{if } -100 \\le \\theta_0 \\le 100  \\\\ 0 & \\text{else}\\end{array}\\right.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4809fccf1e4e2cea8aa848b42f8b2f34",
     "grade": false,
     "grade_id": "cell-85c54c4a189fc811",
     "locked": true,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_flat_prior(theta):\n",
    "    '''\n",
    "    Returns the log uniform prior (-100 <= theta_i <= 100)\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        \n",
    "    Returns:\n",
    "        logPi: (float) log prior (not necessarily normalized)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "def log_symmetric_prior(theta):\n",
    "    '''\n",
    "    Returns the log uniform (for theta_0) and symmetric (for theta_1) prior \n",
    "    \n",
    "    (-100 <= theta_0 <= 100; p(theta_1) \\propto (1+theta_1^2)^(-3/2))\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        \n",
    "    Returns:\n",
    "        logPi: (float) log prior (not necessarily normalized)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(log_flat_prior([0.,0.]), (np.floating, float)), 'The output should be a float'\n",
    "assert isinstance(log_symmetric_prior([0.,0.]), (np.floating, float)), 'The output should be a float'\n",
    "assert log_flat_prior([0.,0.]) - log_flat_prior([10.,-10.]) == 0., 'The flat prior should be constant in the interval'\n",
    "assert np.abs(log_symmetric_prior([-10.,1.]) - log_symmetric_prior([20.,2.]) - 1.3744360978112324) <0.001, \\\n",
    "'The log symmetric prior does not evaluate correctly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd0107dd227ac58207c31983b72d42ca",
     "grade": false,
     "grade_id": "cell-958720f73ee427a3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, dy=sigma):\n",
    "    '''\n",
    "    Returns the log likelihood.\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        x: data (independent variable). array of floats\n",
    "        y: data (dependent variable). array of floats\n",
    "        dy: fixed error (optional; default=sigma defined above), standard deviation of a normal distribution\n",
    "        \n",
    "    Returns:\n",
    "        logL: (float) log likelihood\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test = np.loadtxt(f'{DATA_DIR}/PS2_Prob2_data.txt',unpack=True)\n",
    "assert isinstance(log_likelihood([0,0], x_test, y_test), (np.floating, float)), 'The output should be a float'\n",
    "assert np.abs(log_likelihood([0.,0.], x_test, y_test) - log_likelihood([10.,1.], x_test, y_test) + 16.588440096874997) <0.001, \\\n",
    "'The log likelihood does not evaluate correctly.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Plot the posterior for the two different prior choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks (see also Yata)**\n",
    "* Where is the mode of the posterior with these two different priors?\n",
    "* Plot the joint pdf for the two different prior choices.\n",
    "* Are the parameters correlated / anti-correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24718fe14886b3b1a2920c8961d69f1c",
     "grade": false,
     "grade_id": "cell-ba8d7bc36ae04469",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We'll start by defining a function which takes a two-dimensional grid of likelihoods and \n",
    "# returns 1, 2, and 3-sigma contours. This acts by sorting and normalizing the values and then \n",
    "# finding the locations of the  0.682 ,  0.952 , and  0.9972  cutoffs:\n",
    "def contour_levels(grid):\n",
    "    \"\"\"Compute 1, 2, 3-sigma contour levels for a gridded 2D posterior\"\"\"\n",
    "    _sorted = np.sort(grid.ravel())[::-1]\n",
    "    pct = np.cumsum(_sorted) / np.sum(_sorted)\n",
    "    cutoffs = np.searchsorted(pct, np.array([0.68, 0.95, 0.997]) ** 2)\n",
    "    return np.sort(_sorted[cutoffs])\n",
    "\n",
    "# Optional. \n",
    "# Insert utility code / functions here.\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c594ff32b6a8323cc8ff26774f5f64f",
     "grade": false,
     "grade_id": "cell-97243b6cddebf189",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The dictionary MAP (= maximum a posteriori) should return the mode \n",
    "# of the posterior distribution.\n",
    "# The key is the prior and the value is resulting posterior mode (peak)\n",
    "# given as theta* = [theta0*, theta1*]\n",
    "MAP={}\n",
    "MAP['uniform_prior'] = [0.0, 0.0]\n",
    "MAP['symmetric_prior'] = [0.0, 0.0]\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(MAP[\"uniform_prior\"][1] - 1.977) < 0.01\n",
    "assert np.abs(MAP[\"symmetric_prior\"][0] - 18.725) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e1545461ec1e730d8f98e73236d2de5",
     "grade": false,
     "grade_id": "cell-9e84255bf61c598d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Plot the joint posterior for the two model parameters for the two different priors.\n",
    "- Indicate whether the slope and the intercept are correlated or anti-correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb5168adf85842656189bc7d12d07d58",
     "grade": false,
     "grade_id": "cell-80f2e16a2e4b7897",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: MCMC sampling of a Lorentzian pdf using the random walk Metropolis algorithm (2 points)\n",
    "Note that you must solve this problem if you want to solve (extra) problem 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we have some function that tells us the (possibly unnormalized) probability density for a given position in a one-dimensional space. That is, the function is proportional to a univariate pdf. In general we will not be restricted to univariate distributions, but a key feature of the approach that we will implement here is that it can be straightforwardly extended to many dimensions. \n",
    "\n",
    "In this task we will assume a known, specific form of this univariate pdf, namely a Lorentzian (Cauchy) distribution, but it might just as well be some very complicated function that can only be evaluated numerically. All that is needed is some function that, for each position in the parameter space, returns a number that is proportial to the probability density.\n",
    "\n",
    "Let us start by studying the pdf that we will be sampling from using a random walk (that we will set up using the Metropolis algorithm outlined below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f02d7c456b144508548a5229c58cc78",
     "grade": false,
     "grade_id": "cell-39860bc3703e35b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Modules needed for this exercise\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a number of random samples from the standard Cauchy\n",
    "r = cauchy.rvs(size=1000)\n",
    "plt.hist(r, density=True, histtype='stepfilled', alpha=0.2, \n",
    "         range=(-10,10),bins=21);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This histogram corresponds to a finite sample from the pdf of a standard Cauchy (Lorentzian)\n",
    "$$ \n",
    "p(x | \\alpha=0, \\beta=1) = \\frac{1}{\\pi(1+x^2)}, \n",
    "$$\n",
    "with its mode and median at $\\alpha=0$ and a full-width at half maximum (FWHM) equal to $2\\beta = 2$.\n",
    "\n",
    "Questions to ponder (not part of this problem; answers not requested):\n",
    "- How does this pdf compare with a standard normal distribution $\\mathcal{N}(x;\\mu=0,\\sigma^2=1)$?\n",
    "- The Cauchy distribution is often used in statistics as the canonical example of a \"pathological\" distribution since both its mean value and its variance are undefined. Do you see mathematically why these moments are undefined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Construct a Metropolis sampler for univariate distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata)** First, turn the posterior into a callable function. You should deliberately remove the normalization to make the point that sampling can be made for an unnormalized pdf. Note that we will work directly with the pdf here (not taking the log as in previous examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7df526b99cc5c0235203a4a19de3c312",
     "grade": false,
     "grade_id": "cell-16bc7d26f9be4be2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def posterior_function(x, normalized=False):\n",
    "    '''\n",
    "    Return the posterior pdf given by a standard Cauchy (Lorentzian).\n",
    "    \n",
    "    Args:\n",
    "        x: position in a one-dimensional space\n",
    "        normalized: Return a normalized pdf if True (optional, default=False)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata)** Now on to constructing the sampler. The code for a MCMC sampler that uses the Metropolis algorithm is enclosed below. However, it misses a few critical ingredients and your task is to add these at the correct places.\n",
    "\n",
    "1. At first, you have to specify the initial parameter position (that can be randomly chosen), lets fix it to the input argument `start_position`:\n",
    "\n",
    "```python\n",
    "current_position = start_position\n",
    "```\n",
    "\n",
    "Then, you propose to move (jump) to another position. You can be very dumb or very sophisticated about how you come up with the proposed jump. The Metropolis sampler is rather dumb and just picks a sample from a symmetric proposal distribution (here we again choose a normal distribution) centered around your current position (i.e. `current_position`) with a certain standard deviation (`proposal_width`) that will determine the length scale for proposed jumps \n",
    "\n",
    "2. Use `scipy.stats.norm` to create `proposed_position` as a sample from a random variable that is described by a normal distribution with the mean value equal to the `current_position` and a standard deviation that is given by `proposal_width`.\n",
    "\n",
    "Next, you evaluate whether that new position is a good place to jump to, or not. We quantify this by comparing the probability density at the `proposed_position` with the one for the `current_position`. Usually you would use the logarithms of the probability densities but we omit this here.\n",
    "\n",
    "3. Compute both `p_current` and `p_proposal`.\n",
    "\n",
    "Up until now, we essentially have a hill-climbing algorithm that would propose moves in random directions and only accept the step if the `proposed_position` has higher probability density than `current_position`. Eventually we'll get to the mode (at `x = 0` in this case) from where no more moves will be accepted. However, we want to sample a pdf so we'll also have to sometimes accept moves into regions of lower probability. The key trick is to compute the ratio of the two probabilities,\n",
    "\n",
    "```python\n",
    "p_accept = p_proposal / p_current\n",
    "```\n",
    "\n",
    "and to interpret this ratio as an acceptance probability. Note that the acceptance probability is obtained by dividing the pdf of the proposed parameter setting by the pdf of the current parameter setting. This implies that the probability density does not necessarily need to be normalized, the normalization factor will anyway cancel out. \n",
    "\n",
    "You can see that if `p_proposal` is larger, the acceptance probability will be `> 1` and we'll definitely accept the jump. However, if `p_current` is larger, say twice as large, there'll just be a 50% chance of moving to the proposed position. The acceptance of a proposed step will be decided by sampling another random number.\n",
    "\n",
    "4. Incorporate the acceptance step by comparing `p_accept` to a random number (uniform [0,1]). The `current_position` should be updated if the `accept` variable is `True`. \n",
    "\n",
    "Note that the `current_position` is added to our list of parameter samples at the end of the iteration, regardless of it being a new position or not.\n",
    "\n",
    "This simple procedure gives us samples from the pdf.\n",
    "\n",
    "The code below also prints detailed information if the optional keyword argument `verbose=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e881572709e4f7997afb5858e156d8a",
     "grade": false,
     "grade_id": "cell-30d4e414d6f57b40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sampler(posterior_func, no_of_samples=4, start_position=.5, \n",
    "            proposal_width=1., verbose=False):\n",
    "    '''\n",
    "    Simple (but incomplete) Metropolis sampler function.\n",
    "    \n",
    "    Args:\n",
    "        posterior_func: Function that takes a single positional argument and returns \n",
    "            the (possibly unnormalized) pdf value.\n",
    "        no_of_samples: (integer) Number of samples that will be returned (excluding the start position). \n",
    "            (default=4)\n",
    "        start_position: (float) Start position. (default=0.5)\n",
    "        proposal_width: (float) Width of Gaussian proposal distribution. (default=1.)\n",
    "        verbose: (Boolean) Verbose output (default=False)\n",
    "        \n",
    "    Returns:\n",
    "        samples: Array of floats. Length = no_of_samples+1\n",
    "    '''\n",
    "    # Verbose printing\n",
    "    if verbose:\n",
    "        assert no_of_samples < 11, \"Too many samples for printing\"\n",
    "        print(f'Step    Trace  Proposed  Met.ratio  Accept')\n",
    "        \n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "    samples = [current_position]\n",
    "    for i in range(no_of_samples-1):\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "        # Compute posteriors of current and proposed position       \n",
    "        p_current = posterior_func(current_position)\n",
    "        p_proposal = posterior_func(proposed_position) \n",
    "        \n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "        \n",
    "        # Verbose\n",
    "        if verbose:\n",
    "            assert no_of_samples < 11, \"Too many samples for visualization\"\n",
    "            print(f'{i:>4}  {current_position:7.3f}   {proposed_position:7.3f}    {min(1.,p_accept):5.3f}     {accept}')\n",
    "        \n",
    "        # Possibly update position\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "        \n",
    "        samples.append(current_position)\n",
    "        \n",
    "    return np.array(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell performs some first tests of the functionality of your sampler. It also makes a call to the sampler with the `verbose` option turned on. \n",
    "- How many proposed steps are accepted?\n",
    "- What can we say about the acceptance of proposed steps that would take us closer to the mode of the distribution?\n",
    "- And how about those that are proposed away from the mode?\n",
    "You can re-evaluate this cell multiple times to observe several chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae8f7262a929b1a18cb9d9c9b594aca8",
     "grade": true,
     "grade_id": "cell-2ae464bbbc58c95f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_samples = sampler(posterior_function, no_of_samples=100)\n",
    "assert test_samples[0]==0.5, 'The first position sample should be the default start position = 0.5'\n",
    "assert len(test_samples)==100, 'The sampler chain should contain the specified number of samples'\n",
    "\n",
    "sampler(posterior_function, start_position=0., no_of_samples=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70362bb4de477a566b0e0458661801ad",
     "grade": false,
     "grade_id": "cell-9a2da588961f37ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (b) Sampling the posterior\n",
    "**Task (see also Yata)** Draw 100,000 samples from your sampler and plot:\n",
    "* The trace (i.e. the sequence of draws of your single parameter x)\n",
    "* A normalized histogram of the samples compared to the true posterior pdf (normalized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a30bfa8e49901efaed73bb8fb68df4b",
     "grade": false,
     "grade_id": "cell-b7a2a59b1d878aa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    samples = sampler(posterior_function, no_of_samples=100000, start_position=1.)\n",
    "except:\n",
    "    samples=None\n",
    "    print('The method \"sampler\" must be defined and working as expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9bcca19ee39ddde1205424ae8a97988",
     "grade": false,
     "grade_id": "cell-17e4557f37667278",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting commands here\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "477b0214e8678bf92fd2f379a16b9f47",
     "grade": true,
     "grade_id": "cell-02cd9d30ab8b4c8c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If the code works as expected your trace of samples should have a median value close to 0.0\n",
    "assert np.abs(np.median(samples))<0.2, f'The median of the samples is {np.median(samples):.1}. It should be close to the median of the posterior pdf (=0.0)'\n",
    "# Furthermore, the Cauchy distribution has heavy tails and your chain will surely have ventured out to rather large distances \n",
    "# (providing samples from the tails)\n",
    "assert np.abs(samples).max()>20, f'The furthest sample is {np.abs(samples).max():.1f}. Too small variations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Signal and background (3 points)\n",
    "This problem is more demanding than the other basic problems. Note that you must solve this problem if you want to solve (extra) problem 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this problem is to estimate the amplitude of a signal when there is a background.  We'll take a limiting case where the background is flat, so it is completely specified by its magnitude $B > 0$, and the signal is known to be a Gaussian with unknown amplitude $A$ but known position (mean) and width (standard deviation). \n",
    "\n",
    "The measurements will be integer numbers of counts $\\{N_k\\}$ in well-defined (equally spaced) bins $\\{x_k\\}$. The index $k$ runs over integers labeling the bins.\n",
    "\n",
    "We can imagine three different goals of the data analysis:\n",
    "- Finding $A$ and $B$ given $\\{N_k\\}$.\n",
    "- Finding $A$ (we do not care about $B$).\n",
    "- Finding $B$ (we do not care about $A$).\n",
    "\n",
    "In all cases we consider the bin sizes and the signal shape (including its mean position and width) as known information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our statistical model includes the true signal plus a constant background. The signal and the background magnitudes are the unknown parameters while the other parameters dictating the signal (width $w$ and mean $x_0$ of the Gaussian) are known and fixed:\n",
    "\n",
    "$$\n",
    "   D_k = n_0 \\left[ A e^{-(x_k-x_0)^2/2 w^2} + B \\right]\n",
    "$$\n",
    "\n",
    "Here $n_0$ is a constant that scales with measurement time.  Note that $D_k$ is not an integer in general, unlike $N_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51508b3c6f05fd3a57d7223ab8f6dd50",
     "grade": false,
     "grade_id": "cell-ade2557e6088a291",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import statements.\n",
    "# We use pickle to save and load a python dictionary\n",
    "import pickle\n",
    "# factorial from the math module is useful. You might consider other modules as well.\n",
    "from math import factorial\n",
    "\n",
    "# import additional modules as needed\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "644664b1809f5a1557ece48c4a674325",
     "grade": false,
     "grade_id": "cell-cea19416f4d5f062",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function generates data according to the statistical model\n",
    "A_true = 1.\n",
    "B_true = 2.\n",
    "\n",
    "def exact_data(A, B, n_0, x_k, x_0=0., width=np.sqrt(5.)):\n",
    "    \"\"\"\n",
    "    Return the exact signal plus background.  The overall scale is n_0,\n",
    "    which is determined by how long counts are collected. \n",
    "    The default signal position and width are 0.0 and sqrt(5), respectively (in some  irrelevant units).\n",
    "    \"\"\"\n",
    "    return n_0 * (A * np.exp(-(x_k - x_0)**2/(2.*width**2)) + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisson distribution\n",
    "We are imagining a counting experiment, so the statistics of the counts we record will follow a Poisson distribution. It might be an interesting exercise to derive why this distribution is expected for a counting experiment. \n",
    "\n",
    "The Poisson discrete random variable from scipy.stats is defined by (see [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html))\n",
    "\n",
    "$$\n",
    "p(k \\mid \\mu) = \\frac{\\mu^k e^{-\\mu}}{k!} \\quad \\mbox{for }k\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $k$ is an integer and $\\mu$ is called the shape parameter. The mean and variance of this distribution are both equal to $\\mu$. Sivia and Gregory each use a different notation for for this distribution, which means you need to be flexible. \n",
    "\n",
    "For convenience, we'll define our own version in this notebook:\n",
    "\n",
    "$$\n",
    "p(N \\mid D) = \\frac{D^N e^{-D}}{N!} \\quad \\mbox{for }N\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $N$ is an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f44128714159ffde91418ff139a6f2b0",
     "grade": false,
     "grade_id": "cell-e10dd1c95952f91a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make a dataset for exploring\n",
    "def make_dataset(A_true, B_true, width, x_0, databins=15, delta_x=1, D_max=100):\n",
    "    \"\"\"\n",
    "    Create a data set based on the number of bins (databins), the spacing\n",
    "    of bins (delta_x), and the maximum we want the exact result to have\n",
    "    (D_max, this fixes the n_0 parameter).\n",
    "    \n",
    "    Return arrays for the x points (xk_pts), the corresponding values of the\n",
    "    exact signal plus background in those bins (Dk_pts), the measured values\n",
    "    in those bins (Nk_pts, integers drawn from a Poisson distribution), the \n",
    "    maximum extent of the bins (x_max) and n_0.\n",
    "    \"\"\"\n",
    "    # set up evenly spaced bins, centered on x_0\n",
    "    x_max = x_0 + delta_x * (databins-1)/2\n",
    "    xk_pts = np.arange(-x_max, x_max + delta_x, delta_x, dtype=int)\n",
    "    \n",
    "    # scale n_0 so maximum of the \"true\" signal plus background is D_max\n",
    "    n_0 = D_max / (A_true + B_true)  \n",
    "    Dk_pts = exact_data(A_true, B_true, n_0, xk_pts, width=width)\n",
    "    \n",
    "    # sample for each k to determine the measured N_k\n",
    "    Nk_pts = [stats.poisson.rvs(mu=Dk) for Dk in Dk_pts]\n",
    "    \n",
    "    return xk_pts, Dk_pts, Nk_pts, x_max, n_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the signal and the data (these tasks are not graded but will help you to understand the problem)\n",
    "* Make a plot of the true signal plus background we are trying to deduce. Use $A_\\mathrm{true}=1$ and $B_\\mathrm{true}=2$ and the signal position (mean) $x_0=0$ and width (standard deviation)  $w=\\sqrt{5}$.\n",
    "\n",
    "We consider what happens for fixed signal and background but changing the experimental conditions specified by `D_max` and `databins` (we'll keep `delta_x` fixed to 1). In all cases the bins are symmetric around $x=0$.\n",
    "\n",
    "The pickle file that is loaded in the cell below contains data from four differently designed counting experiments.:\n",
    "1. Baseline case: 15 bins and maximum expection of 100 counts per bin.\n",
    "1. Low statistics case: 15 bins and maximum expection of only 10 counts per bin.\n",
    "1. Greater range case: 31 bins (with fixed bin width) and maximum expection of 50 counts per bin to give approximately the same total number of counts as in baseline case.\n",
    "1. Smaller range case: 7 bins (with fixed bin width) and maximum expection of 200 counts per bin to give approximately the same total number of counts as in baseline case.\n",
    " \n",
    "* Make four subplots that correspond to the data from the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4145094cc5862704b16de29e927a54c",
     "grade": false,
     "grade_id": "cell-b300bc6fd3ee1f07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded \"data\" dictionary from file.\n",
      "Extract data with:\n",
      "xk_pts, Dk_pts, Nk_pts, x_max, n_0 = data[case]\n",
      "where the key \"case\" is one of:\n",
      "dict_keys(['Baseline', 'Low Statistics', 'Greater Range', 'Smaller Range'])\n"
     ]
    }
   ],
   "source": [
    "# Plotting the signal and background\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# \n",
    "import pickle\n",
    "\n",
    "# The data has been generated already and will be loaded from a pickle file.\n",
    "# It is a dictionary with four keys corresponding to the four cases, and each value\n",
    "# corresponding to xk_pts, Dk_pts, Nk_pts, x_max, n_0\n",
    "with open(f'{DATA_DIR}/PS2_Prob4_data.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    print('Loaded \"data\" dictionary from file.')\n",
    "    print('Extract data with:')\n",
    "    print('xk_pts, Dk_pts, Nk_pts, x_max, n_0 = data[case]')\n",
    "    print('where the key \"case\" is one of:')\n",
    "    cases = data.keys()\n",
    "    print(cases)\n",
    "\n",
    "# Plotting the data for the four cases\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08a0ca5360d4bad5af09dd37963a9366",
     "grade": false,
     "grade_id": "cell-d1a0b4b6a44f1f7b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### (a) Compute and plot the joint posterior on a grid\n",
    "**Tasks (see also Yata)**\n",
    "* Implement functions for the (log) likelihood and for a uniform (log) prior. Let's use a uniform prior for $0 \\le A \\le 5$ and $0 \\le B \\le 5$.\n",
    "* Evaluate the log-posterior on a grid and then: \n",
    "  - Plot the joint posterior pdf for $A$ and $B$ for the four cases.\n",
    "  - Plot the marginalized posterior pdf for the signal amplitude $A$ for the four cases.\n",
    "  \n",
    "  Use the same axis scales for all four cases such that the precision of the inference can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a1f8ba7ff4c390ee59ae590ca751f0a",
     "grade": false,
     "grade_id": "cell-a3772dfb9786e84d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the pdfs and combine with Bayes' theorem.\n",
    "\n",
    "def log_prior(A, B):\n",
    "    \"\"\"\n",
    "    Log prior .\n",
    "    \n",
    "    We take a uniform (flat) prior with large enough\n",
    "    maximums but, more importantly, require positive values for A and B.\n",
    "    \"\"\"\n",
    "    A_max = 5.\n",
    "    B_max = 5.\n",
    "    # flat prior \n",
    "    if np.logical_and(A <= A_max, B <= B_max).all(): \n",
    "        return np.log(1/(A_max * B_max))\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "\n",
    "def log_likelihood(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log likelihood for data Nk_pts given A and B\"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "def log_posterior(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log posterior for data Nk_pts given A and B\"\"\"\n",
    "    return log_prior(A, B) + log_likelihood(A, B, xk_pts, Nk_pts, n_0)\n",
    "\n",
    "# Other utility code can be put here (if needed)\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cb9360be5de8932f51c3531dcaaa9dd",
     "grade": false,
     "grade_id": "cell-cbb85be603215ec4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Optional.\n",
    "# Code to find contour levels of gridded 2D posterior.\n",
    "\n",
    "def find_contour_levels(grid):\n",
    "    \"\"\"Compute 1, 2, 3-sigma contour levels for a gridded 2D posterior\n",
    "       Note: taken from BayesianAstronomy but may not work here.\n",
    "    \"\"\"\n",
    "    sorted_ = np.sort(grid.ravel())[::-1]\n",
    "    pct = np.cumsum(sorted_) / np.sum(sorted_)\n",
    "    cutoffs = np.searchsorted(pct, np.array([0.68, 0.95, 0.997]) ** 2)\n",
    "    return np.sort(sorted_[cutoffs])\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11f0ba8e4b6bee89a8c5268cdb67cc35",
     "grade": false,
     "grade_id": "cell-219fdc3158a3dc82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (b) Design of experiment\n",
    "**Tasks (see also Yata)** \n",
    "Answer the following questions. Some of them are repeated on Yata.\n",
    "  1. Can you understand why the signal and background amplitudes are anticorrelated? And why the (anti)correlation seems to be stronger in one of the cases? \n",
    "  1. What are your conclusions for how to design the experiment given limited resources? \n",
    "    - In particular, given that you wanted to be able to distinguish between signal amplitude and background, would it then be better to have many counts in few bins, or the same total amount of counts spread over a wider interval? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f047ae46ddc8ee3d71001c28128e322a",
     "grade": true,
     "grade_id": "cell-620bd3518d4f7363",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
