{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- See deadline on the course web page\n",
    "- This problem set is solved individually. See examination rules on the course web page and the explanation of the examination procedure below.\n",
    "- The two notebooks for each problem set contain a number of basic and extra problems; you can choose which and how many to work on. The extra problems are usually more challenging.\n",
    "- Students are allowed to discuss together and help each other when solving the problems. However, every student must understand their submitted solution in the sense that they should be able to explain and discuss them with a peer or with a teacher.\n",
    "- While discussions with your peers are allowed (and even encouraged), direct plagiarism is not. Every student must reach their own understanding of submitted solutions according to the definition in the previous point.\n",
    "- The use of coding assistance from code generating artificial intelligence tools is allowed. However, every student must reach their own understanding of submitted solutions (including employed algorithms) according to the definition above.\n",
    "- Some problems include checkpoints in the form of `assert` statements. These usually check some basic functionality and you should make sure that your code passes these statements without raising an `AssertionError`. \n",
    "- Do not use other python modules than the ones included in the `environment.yml` file in the course github repo. \n",
    "\n",
    "- **Important:** The grading of problem sets requires **all** of the following actions:\n",
    "  1. Make sure to always complete **Task 0** in the header part of the notebook and that this part does not raise any `AssertionError`(s).\n",
    "  1. **Complete** the corresponding questions in Yata for every task that you have completed. This usually involves copying and pasting some code from your solution notebook and passing the code tests. You need to have a green check mark on Yata to get the corresponding points.\n",
    "  1. **Upload** your solution in the form of your edited version of this Jupyter notebook via the appropriate assignment module in Canvas (separate for basic and extra tasks). It is the code and results in your submitted notebook that is considered to be your hand-in solution.\n",
    "  1. If selected, be **available for a discussion** of your solution with one of the teachers on the Monday afternoon exercise session directly following the problem set deadline. No extra preparation is needed for these discussions apart from familiarity with your own solution. A list of randomly selected students will be published on the course web page around Monday noon. During the afternoon session that same day, students will be called in the numbered order until the end of the list (or the end of the exercise session). You must inform the responsible teacher as soon as possible following the publication of the student list if you can not be physically present at the exercise session (in which case we will have the discussion on zoom). An oral examination (on all aspects of the course) will be arranged during the exam week for students that do not show up for their discussion slot, or that fail to demonstrate familiarity with their hand-in solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "- Make sure that the **run time is smaller than a few minutes**. If needed you might have to reduce some computational tasks; e.g. by decreasing the number of grid points or sampling steps. Please ask the supervisors if you are uncertain about the run time. \n",
    "\n",
    "- Your solutions are usually expected where it says `YOUR CODE HERE` or <font color=\"red\">\"PLEASE WRITE YOUR ANSWER HERE\"</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 \n",
    "#### (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "251105c400969a6d24e9f7ec4883888e",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "- All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64a7020fd0e62e5b51ef4470ae1c797f",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student_self_assessment = False\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 (Basic problems)\n",
    "**Learning from data [TIF285], Chalmers, Fall 2025**\n",
    "\n",
    "Last revised: 16-Sep-2025 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c532e9c42f0ac2234f7eb5f88c73672b",
     "grade": false,
     "grade_id": "cell-f3768fbcb502fd03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Data files are stored in\n",
    "DATA_DIR = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Coin tossing (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata):** Define a function `bayesian_analysis_coin_flips` (see start code below) that returns the mean, median, and 68%/95% credible intervals of the Bayesian posterior with an input data array of coin flips and using a uniform $[0,1]$ prior for `pH` (probability of heads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (mainly for testing your code; not on Yata)** Read the data with simulated coin tosses from the file `cointosses.dat`.\n",
    "Each row corresponds to a single toss: 0=tails; 1=heads\n",
    "\n",
    "Extract the mean and the 95% credible intervals (Degree-of-belief or DoB intervals) from the first 8 tosses, the first 64 tosses, the first 512 tosses and all 4096 tosses in the data assuming a uniform prior for the probability $p_H$ of obtaining heads in a single toss.\n",
    "\n",
    "*Hint*: Sample code for computing the DoB interval is available in the lecture notes: \"Demonstration: Bayesian Coin Tossing\". The tests below will fail if you use a too sparse grid when assigning point esitmates and credible intervals. Use at least 1000 mesh points for evaluating $p(p_H|D,I)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata):** Check how the width ($d_{68}$) of your 68% credible interval depends on the number of coin tosses ($N$) that is included in the data likelihood. It should be some power-law dependence: $d_{68} \\propto N^p$; but which power $p$? What do you expect and what do you find?\n",
    "\n",
    "*Hint*: You might want to make a logarithmic plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edcbcf45dfdc39223346180085913d1b",
     "grade": false,
     "grade_id": "cell-f27c4016e4570c15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#...\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd34410ae6a9662eac24de21527e4929",
     "grade": false,
     "grade_id": "cell-71d9ceb95f1f80db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = np.loadtxt(f'{DATA_DIR}/PS2_Prob1_cointosses.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cc3e4665a66c30f9b147ce475a5973e",
     "grade": false,
     "grade_id": "cell-79bec52ebb8e7d12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional. \n",
    "# Insert utility code / functions here.\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9debbb5c3eb0dd6c23b451c5e8452c6c",
     "grade": false,
     "grade_id": "cell-538f840bea75b7dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that returns the mean, median, and 68%/95% credible intervals \n",
    "# of the Bayesian posterior with an input data array of coin flips \n",
    "# and using a uniform [0,1] prior for the pH (probability of heads).\n",
    "\n",
    "def bayesian_analysis_coin_flips(data_coin_tosses, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns various Bayesian analysis results for the given data of coin tosses.\n",
    "    \n",
    "    The posterior is p( pH | data, I).\n",
    "    Assume a uniform p(pH|I) = U[0,1] prior\n",
    "    \n",
    "    Args:\n",
    "        data_coin_tosses: Array of shape (m,) with 'm' independent binary data.\n",
    "            0 = tails; 1 = heads\n",
    "        verbose: Print the results (default=False)\n",
    "            \n",
    "    Returns:\n",
    "        (mean, mode, median, dob68, dob95): A tuple with the following elements\n",
    "            mean: The mean of the posterior distribution (float)\n",
    "            mode: The mode of the posterior distribution (float)\n",
    "            median: The median of the posterior distribution (float)\n",
    "            dob68: A tuple (lo,hi) with the lower and upper limits of the \n",
    "                68% degree-of-belief range of the posterior distribution (float,float)\n",
    "            dob95: A tuple (lo,hi) with the lower and upper limits of the \n",
    "                95% degree-of-belief range of the posterior distribution (float,float)\n",
    "    \"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48b14dcc1063a63a623c875494a7fc58",
     "grade": true,
     "grade_id": "cell-a567356cf18d3da5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:1])\n",
    "for output in (mean, mode, median, dob68[0], dob95[0]):\n",
    "    assert output.dtype=='float64', 'Wrong type'\n",
    "assert len(dob68)==2, 'DoB tuple should be of length 2'\n",
    "assert len(dob95)==2, 'DoB tuple should be of length 2'\n",
    "assert np.abs(mean-0.667)<0.001\n",
    "assert np.abs(mode-1.0)<0.001\n",
    "assert np.abs(median-0.707)<0.001\n",
    "\n",
    "# 8 tosses\n",
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:8])\n",
    "assert np.abs(mode-0.5)<0.001\n",
    "assert np.abs(dob68[0]-0.3429)<0.001\n",
    "assert np.abs(dob68[1]-0.6571)<0.001\n",
    "# 1024 tosses\n",
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:1024])\n",
    "assert np.abs(median-0.4863)<0.001\n",
    "# 4096 tosses\n",
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:4096])\n",
    "assert np.abs(dob95[0]-0.4591)<0.001\n",
    "assert np.abs(dob95[1]-0.4897)<0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "694a253c76490aa008e165166b68f13a",
     "grade": false,
     "grade_id": "P1-width-dependence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Explore the dependence of the width of the 95% DoB on the number of coin tosses.\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: The lighthouse problem (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A light house is located at position $\\alpha$ along a straight shoreline and a distance $\\beta$ out at sea. It cannot be seen in the dark, but it emits lightpulses at random intervals (and therefore in random directions). Photodetectors on the shore detects the positions of the arrival of the falshes, but it cannot measure its incoming direction. Given the positions of $N$ such flashes\n",
    "$$\n",
    "\\mathcal{D} = \\{ x_k \\}_{k=1}^N,\n",
    "$$\n",
    "what would you infer for the position $(\\alpha, \\beta)$ of the lighthouse?\n",
    "\n",
    "- You can assume that the directions (azimuths) of the flashes are independent and identically distributed random variables given by a uniform distribution $\\theta_k \\sim \\mathcal{U}\\left( \\pi/2, \\pi/2\\right)$.\n",
    "- Trigonometry relates $\\beta \\tan\\theta_k = (x_k - \\alpha)$ and you can use a change of variables to write the likelihood of one observation\n",
    "  $$\n",
    "  p(x_k \\vert \\alpha, \\beta, I) = p(\\theta_k \\vert \\alpha, \\beta, I) \\left| \\frac{d\\theta_k}{d x_k} \\right| = \\ldots = \\frac{\\beta}{\\pi\\left[ \\beta^2 + (x_k - \\alpha)^2\\right]}.\n",
    "  $$\n",
    "- You can use simple, uniform priors for $(\\alpha, \\beta)$. Assume that the prior ranges are restricted to $-5.0 \\leq \\alpha \\leq +5.0$ and $0.1 \\leq \\beta \\leq 5.0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1f5c4b4a5b24bd97f29675aedc40c5a",
     "grade": false,
     "grade_id": "cell-9a2da588961f37ab12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (a) Evaluate the (log) posterior.\n",
    "**Tasks (see also Yata)** \n",
    "* Define functions `logPrior`, `logLikelihood`, and `logPosterior` that can evaluate these PDF:s on a grid of $(\\alpha, \\beta)$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdea4d8d0b727706dc0a10ed715b0a07",
     "grade": false,
     "grade_id": "P2-generate-data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing modules if needed...\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "xpositions = np.loadtxt(f'{DATA_DIR}/PS2_Prob2_xpositions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "413d22348ca8459d56ddfdadce1b6280",
     "grade": false,
     "grade_id": "P2-pdfs",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def logPrior(theta):\n",
    "    '''\n",
    "    Returns the log uniform prior (-5. <= alpha <= 5., 0.1 <= beta <= 5.) for a grid of theta.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    theta : ndarray, shape=(Nsamples,2)\n",
    "        theta[:,0] = alpha, theta[:,1] = beta\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    logPi : ndarray(Nsamples,)\n",
    "        log prior (not necessarily normalized) for the grid of theta.\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "\n",
    "def logLikelihood(theta, x):\n",
    "    '''\n",
    "    Returns the log likelihood for a grid of theta.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    theta : ndarray, shape(Nsamples, 2)\n",
    "        theta[:,0] = alpha, theta[:,1] = beta\n",
    "    x : nddata, shape=(Ndata,)\n",
    "        array of positions for measured flashes\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    logL : ndarray, shape=(Nsanples,) \n",
    "        log likelihood (not necessarily normalized) for the grid of theta.\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "def logPosterior(theta, x):\n",
    "    '''\n",
    "    Return the log posterior (not necessarily normalized).\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests of the prior\n",
    "assert isinstance(logPrior(np.ones((1,2)))[0], (np.floating, float)), \\\n",
    "    'The output elements of the prior should be floats'\n",
    "assert logPrior(np.zeros((1,2)))==-np.inf, \\\n",
    "    'The prior = 0 for beta=0.'\n",
    "assert logPrior(np.ones((5,2))).shape == (5,), \\\n",
    "    'The prior output should be of shape (5,) with 5 input positions.'\n",
    "# Tests of the likelihood\n",
    "assert isinstance(logLikelihood(np.ones((1,2)),xpositions[:256])[0], (np.floating, float)), \\\n",
    "    'The output elements of the likelihood should be floats'\n",
    "assert logLikelihood(np.ones((5,2)),xpositions).shape == (5,), \\\n",
    "    'The likelihood output should be of shape (5,) with 5 input positions and data.'\n",
    "# Tests of the posterior\n",
    "assert(np.isclose((logPosterior(np.ones((1,2)),xpositions[:4]) - logPosterior(0.5*np.ones((1,2)),xpositions[:4]) \\\n",
    "                       + logPosterior(np.ones((1,2)),xpositions[:2]) - logPosterior(0.5*np.ones((1,2)),xpositions[:2]))[0], 3.88864673)), \\\n",
    "       'The log posterior has an incorrect dependence on the observations.'\n",
    "assert(np.isclose((logPosterior(np.ones((1,2)),xpositions) - logPosterior(2*np.ones((1,2)),xpositions))[0], -295.0531449)), \\\n",
    "        'The log posterior has an incorrect dependence on the position.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8f2814e9add54c1fa3f536d9d294850",
     "grade": false,
     "grade_id": "cell-9a2da588961f37ab13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (b) Evaluate the (log) posterior on a grid and find the mode.\n",
    "**Tasks (see also Yata)** \n",
    "* Create a grid of $(\\alpha, \\beta)$ values and evaluate the log posterior for different amounts of available data. The data is loaded into the array `xpositions` in the cell below. Make a 3x3 figure with `subplots` and plot the posterior (not the log!) for the $N = (2, 3, 4, 8, 16, 32, 64, 128, 1024)$ first observations. You need to be careful in choosing the number of grid points in each direction as the number of evaluations scales quadratically with this number.\n",
    "* You should use the supplied method `contour_levels` to extract the isoprobability levels that define (0.68, 0.95, 0.997) credible regions. This method takes a 2D grid of PDF values (not necessarily normalized) as an input. These levels can be plotted using the `contour` method.\n",
    "* Use `scipy.optimize` to find the mode of the posterior for the full set of observations with four significant digits. Which coordinate is inferred with the best precision from the observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbfb23ea9839dc551d071c5af4f0e05e",
     "grade": false,
     "grade_id": "P2-plots",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a grid and evaluate the log-posterior. Plot the bivariate posterior PDF.\n",
    "\n",
    "# We'll start by defining a function which takes a two-dimensional grid of probability densities and \n",
    "# returns 1, 2, and 3-sigma contours. This acts by sorting and normalizing the values and then \n",
    "# finding the locations of the  0.682 ,  0.952 , and  0.9972  cutoffs:\n",
    "def contour_levels(grid):\n",
    "    \"\"\"Compute 1, 2, 3-sigma contour levels for a gridded 2D pdf\"\"\"\n",
    "    _sorted = np.sort(grid.ravel())[::-1]\n",
    "    pct = np.cumsum(_sorted) / np.sum(_sorted)\n",
    "    cutoffs = np.searchsorted(pct, np.array([0.68, 0.95, 0.997]) ** 2)\n",
    "    return np.sort(_sorted[cutoffs])\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2717dbf53d6179e9bc94135cf8a5cc72",
     "grade": false,
     "grade_id": "P2-mode",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Find the MAP point (maximum a posteriori) for full data set.\n",
    "#\n",
    "# You might want to use functionality from scipy to find the mode.\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: MCMC sampling using the random walk Metropolis algorithm (3 points)\n",
    "Note that you have to have solved problem 2 before starting on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we have some function that tells us the (possibly unnormalized) probability density for a given position in a multi-dimensional space. That is, the function is proportional to a multivariate pdf. \n",
    "\n",
    "Note that we will work with the log posterior and you will have to be careful when computing the acceptance ratio.\n",
    "\n",
    "In this task we will use the (log) posterior pdf from problem 2 but the approach is very general. All that is needed is some function that, for each position in the parameter space, returns a number that is proportial to the (log) probability density. You should deliberately remove all normalization constants to make the point that sampling can be made for an unnormalized pdf. \n",
    "\n",
    "Note that the `log_posterior_func` used as input to the sampler should just take an array of the position $(\\alpha,\\beta)$ as input. The very simple function below achieves this using the `logPosterior` function from Problem 2. Note that we will sample the posterior obtained with N=128 observations and that we just need to evaluate for one position at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logPosteriorFunc(theta):\n",
    "    return logPosterior(theta.reshape(1,2), xpositions[:128])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Construct a Metropolis sampler for the bivariate posterior distribution from problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata)** Now on to constructing the sampler. The code for a MCMC sampler that uses the Metropolis algorithm is enclosed below. However, it misses a few critical ingredients and your task is to add these at the correct places.\n",
    "\n",
    "1. At first, you have to specify the initial parameter position (that can be randomly chosen), lets fix it to the input argument `start_position`:\n",
    "\n",
    "```python\n",
    "current_position = start_position\n",
    "```\n",
    "\n",
    "Then, you propose to move (jump) to another position. You can be very dumb or very sophisticated about how you come up with the proposed jump. The Metropolis sampler is rather dumb and just picks a sample from a symmetric proposal distribution.\n",
    "\n",
    "2. Use `scipy.stats.multivariate_normal` to create `proposed_position` as a sample from a random variable that is described by a multivariate normal distribution with the mean value equal to the `current_position` and a diagonal covariance matrix with fixed standard deviation in all dimensions (`proposal_width`) that will determine the length scale for proposed jumps. Note that you must include the keyword argument `seed=rng` to use the input Generator to draw random numbers.\n",
    "\n",
    "Next, you evaluate whether that new position is a good place to jump to, or not. We quantify this by comparing the probability density at the `proposed_position` with the one for the `current_position`. Note that you are computing the logarithms of the probability densities but the ratio is for the probability densities at the two positions.\n",
    "\n",
    "3. Compute both `logp_current` and `logp_proposal` and from them the acceptance probability `p_accept`.\n",
    "\n",
    "Note that the acceptance probability is obtained by dividing the pdf of the proposed parameter setting by the pdf of the current parameter setting. This implies that the probability density does not necessarily need to be normalized, the normalization factor will anyway cancel out. \n",
    "\n",
    "You can see that if `p_proposal` is larger, the acceptance probability will be `> 1` and we'll definitely accept the jump. However, if `p_current` is larger, say twice as large, there'll just be a 50% chance of moving to the proposed position. The acceptance of a proposed step will be decided by sampling another random number.\n",
    "\n",
    "4. Incorporate the acceptance step by comparing `p_accept` to a random number (uniform [0,1]). The `current_position` should be updated if the `accept` variable is `True`. \n",
    "\n",
    "Note that the `current_position` is added to our list of parameter samples at the end of the iteration, regardless of it being a new position or not.\n",
    "\n",
    "This simple procedure gives us samples from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "791ca2c0b3805f40f9e0d3e0ff930539",
     "grade": false,
     "grade_id": "P3-sampler",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Modules\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def sampler(log_posterior_func, Ndim, no_of_samples=4, start_position=[], \n",
    "            proposal_width=1., rng=np.random.default_rng()):\n",
    "    '''\n",
    "    Metropolis sampler function for a multi-dimensional pdf.\n",
    "    \n",
    "    Args:\n",
    "        log_posterior_func: Function \n",
    "            Function that takes `Ndim` positional arguments and returns \n",
    "            the logarithm of the (possibly unnormalized) pdf.\n",
    "        Ndim: int\n",
    "            Number of parameter dimensions\n",
    "        no_of_samples: int \n",
    "            Number of samples that will be returned (including the start position). \n",
    "        start_position: array_like, default []\n",
    "            Start position. Set as np.ones(Ndim) if start_position==[]\n",
    "        proposal_width: float \n",
    "            Width of symmetric Gaussian proposal distribution.\n",
    "        rng: Generator, default np.random.default_rng()\n",
    "            A random number generator that will be used for all random number samples.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray (no_of_samples,Ndim)\n",
    "            MCMC samples\n",
    "    '''\n",
    "    # starting parameter position\n",
    "    # Set the start position as np.ones(Ndim) if start_position=[]\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "    samples = current_position\n",
    "    \n",
    "    # fixed covariance matrix\n",
    "    cov = np.eye(len(current_position))*proposal_width**2\n",
    "    \n",
    "    # Sampling loop\n",
    "    for i in range(no_of_samples-1):\n",
    "        # suggest new position\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "        # Compute posteriors of current and proposed position   \n",
    "        # Also compute the acceptance probability\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "        \n",
    "        # Accept proposal?\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "        \n",
    "        # Possibly update position\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "        \n",
    "        samples = np.vstack((samples,current_position))\n",
    "        \n",
    "    return np.array(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell performs some first tests of the functionality of your sampler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5677942558f4932313fd935ff947233",
     "grade": true,
     "grade_id": "cell-2ae464bbbc58c95f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tests of the sampler functionality\n",
    "#\n",
    "# One-dimensional test PDF\n",
    "#\n",
    "def log_posterior_function_1d_test(x):\n",
    "    '''\n",
    "    Return the log posterior pdf given by a standard Cauchy (Lorentzian). Not normalized.\n",
    "    \n",
    "    Args:\n",
    "        x: position in a one-dimensional space\n",
    "    '''\n",
    "    return np.log( 1 / (1+x**2) )\n",
    "\n",
    "my_rng = np.random.default_rng(seed=2020)\n",
    "test_samples = sampler(log_posterior_function_1d_test, 1, no_of_samples=5, \\\n",
    "                       start_position=[.5],rng=my_rng)\n",
    "\n",
    "assert len(test_samples)==5, 'The sampler chain should contain the specified number of samples'\n",
    "assert test_samples[0]==0.5, 'The first position sample should be the default start position = 0.5'\n",
    "assert test_samples[1]==0.5, 'The second position will be the same as the start position given this RNG.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ec8280a67f4f2589b912525d343b458",
     "grade": true,
     "grade_id": "P3-2dtests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tests of the sampler with the posterior from Problem 2\n",
    "#\n",
    "my_rng = np.random.default_rng(seed=2020)\n",
    "test_samples_2D = sampler(logPosteriorFunc, 2, no_of_samples=5, \\\n",
    "                       start_position=[],rng=my_rng)\n",
    "\n",
    "assert test_samples_2D.shape==(5,2), 'The sampler chain should contain the specified number of samples'\n",
    "assert (test_samples_2D[0,:]==np.ones(2)).all(), 'The first position sample should be the default start position. '\n",
    "assert (test_samples_2D[2,:]!=np.ones(2)).all(), 'The third position will be different than the start position given this RNG.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c3a35f613807e370a9e13240bf90f21",
     "grade": false,
     "grade_id": "cell-9a2da588961f37ab14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (b) Sampling the posterior\n",
    "**Task (see also Yata)** Draw 100,000 samples from your sampler:\n",
    "* Plot the traces (i.e. the sequence of draws of the two parameters $(\\alpha, \\beta)$.)\n",
    "* Plot the bivariate posterior PDF as well as the univariate marginal distributions for each parameter. You can use the `corner` package to achieve this. A sample code is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "248e15a9053f5f4ac23ee2f4faca39d9",
     "grade": false,
     "grade_id": "cell-b7a2a59b1d878aa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "my_proposal_width=0.5\n",
    "\n",
    "try:\n",
    "    samples = sampler(logPosteriorFunc, 2, no_of_samples=100000, proposal_width=my_proposal_width, start_position=[])\n",
    "except:\n",
    "    samples=None\n",
    "    print('The method \"sampler\" must be defined and working as expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7b3dd599af1bed4e3773c384512d80d",
     "grade": false,
     "grade_id": "cell-17e4557f37667278",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the traces\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21c01fbf28ad4880a938d50a856ac847",
     "grade": true,
     "grade_id": "cell-02cd9d30ab8b4c8c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If the code works as expected your trace of samples should have a median value close to 2.102, 2.3781\n",
    "assert np.isclose(np.median(samples, axis=0)[0],2.102, rtol=0.01), \\\n",
    "    f'The median of the alpha samples is {np.median(samples, axis=0)[0]:5.2f}. It should be close to the median of the posterior pdf.'\n",
    "assert np.isclose(np.median(samples, axis=0)[1],2.378, rtol=0.01), \\\n",
    "    f'The median of the beta samples is {np.median(samples, axis=1)[0]:5.2f}. It should be close to the median of the posterior pdf.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code to make a corner plot with dashed lines indicating 16%, 50%, 84% quantiles\n",
    "# i.e. showing the median and the 68% equal-tail interval\n",
    "\n",
    "# Samples from a correlated, bivariate normal distribution\n",
    "my_mean = np.array([1., 0.5])\n",
    "my_cov = np.array([[0.5, 0.1],[0.1,0.25]])\n",
    "mysamples = stats.multivariate_normal(mean=my_mean, cov=my_cov).rvs(size=10000)\n",
    "\n",
    "import corner\n",
    "corner.corner(mysamples,labels=[r\"$\\theta_1$\", r\"$\\theta_2$\"],show_titles=True,\n",
    "                       quantiles=[0.16, 0.5, 0.84],);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63944ed2436979df26f75e48e804a1c9",
     "grade": false,
     "grade_id": "P3b-corner",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the posterior PDF from your MCMC samples\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f71b3e3bbecdea1d323f16ff181f329",
     "grade": false,
     "grade_id": "cell-9a2da588961f37ab15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (c) Autocorrelation length\n",
    "\n",
    "This problem might be more demanding than (a) and (b) since it requires reading up on autocorrelation and the extraction of an autocorrelation length.\n",
    "\n",
    "A challenge when doing MCMC sampling is that the collected samples can be *correlated*. This can be tested by computing the *autocorrelation function* and extracting the correlation time for each dimension in a chain of samples.\n",
    "\n",
    "Say that $X$ is an array of $N$ samples of one parameter numbered by the index $t$. Then $X_{+h}$ is a shifted version of $X$ with elements $X_{t+h}$. The integer $h$ is called the *lag*. Since we have a finite number of samples, the array $X_{+h}$ will be $h$ elements shorter than $X$. \n",
    "\n",
    "Furthermore, $\\bar{X}$ is the average value of $X$.\n",
    "\n",
    "We can then define the autocorrelation function $\\rho(h)$ from the list of samples. \n",
    "$$\n",
    "\\rho(h) = \\frac{\\sum_{t=0}^{N-h-1} \\left[ (X_t - \\bar{X}) (X_{t+h} - \\bar{X})\\right]}\n",
    "{\\sqrt{ \\sum_{t=0}^{N-h-1} (X_t - \\bar{X})^2 } \\sqrt{ \\sum_{t=0}^{N-h-1} (X_{t+h} - \\bar{X})^2 }}\n",
    "$$\n",
    "The summation is carried out over the subset of samples that overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is often observed that $\\rho(h)$ is roughly exponential so that we can define an autocorrelation time $\\tau$ according to\n",
    "$$\n",
    "\\rho(h) \\sim \\exp(-h/\\tau).\n",
    "$$\n",
    "* Try to understand what the autocorrelation time measures and why it is large (small) when the samples are correlated (not correlated). A short explanation of the autocorrelation function is found below\\*.\n",
    "\n",
    "\\* *The autocorrelation is the overlap (scalar product) of the chain of samples (the trace) with a copy of itself shifted by the lag, as a function of the lag. If the lag is short so that nearby samples are close to each other (and have not moved very far) the product of these two vectors is large. If samples are independent, you will have both positive and negative numbers in the overlap that cancel each other.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata)** \n",
    "* Implement functions to compute the acceptance ratio and the autocorrelation for a  chain of MCMC samples.\n",
    "* Study the samples of the posterior from task b and extract both the autocorrelation time $\\tau$ and the acceptance ratio $r$ for $\\alpha$ and $\\beta$. While not required, it is recommended to verify your extracted autocorrelation times by plotting the computed autocorrelation together with the exponential decay $\\exp(-h/\\tau)$.\n",
    "* Restrict the sampling to 10,000 samples and use a max lag of 2,000.\n",
    "* Use some different choices of the proposal width: 0.01, 0.05, 0.1, 0.5. What is a good choice for sampling this distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94cca9a4ed19f954d2fdc2c680682db2",
     "grade": false,
     "grade_id": "P3c-acc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def acceptance_ratio(chain):\n",
    "    '''\n",
    "    Returns the acceptance ratio for a MCMC chain\n",
    "    \n",
    "    Args:\n",
    "        chain: ndarray with MCMC samples. The length of axis-0 \n",
    "            corresponds to the number of samples and the length of axis-1\n",
    "            to the number of parameters\n",
    "            \n",
    "    Returns:\n",
    "        r: Acceptance ratio (float). Note that 0 <= r <= 1.0\n",
    "            We define r = number of accepted proposed steps / # proposed steps\n",
    "            where we note that the # propsed steps = # samples - 1\n",
    "    '''\n",
    "    \n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this one-dimensional chain we have accepted two proposed steps out of four (the first one is the starting point)\n",
    "chain_1d = np.array([0., 0., 1., 2., 2.])\n",
    "assert np.isclose( acceptance_ratio(chain_1d), 0.5)\n",
    "# In this two-dimensional chain we have accepted two proposed steps out of three (the first one is the starting point)\n",
    "chain_2d = np.array([[0., 0.], [1., 2.], [1., 2.], [2., 2.]])\n",
    "assert np.isclose( acceptance_ratio(chain_2d), 2./3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c938ed654b8c0ecf12adb228169bba1f",
     "grade": false,
     "grade_id": "P3c-autocorr",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def autocorrelation(chain, max_lag):\n",
    "    \"\"\"\n",
    "    Autocorrelation function rho(h) for a MCMC chain.\n",
    "    \n",
    "    Args:\n",
    "        chain: ndarray with MCMC samples. The length of axis-0 \n",
    "            corresponds to the number of samples, n_samples, and the length of axis-1\n",
    "            to the number of parameters, ndim.\n",
    "        max_lag: The maximum lag, h. (integer)\n",
    "        \n",
    "    Returns:\n",
    "        acors: ndarray containing the autocorrelations rho(h) for each\n",
    "                dimension of the chain separately.\n",
    "\n",
    "    The shape of the returned array is\n",
    "        -> (max_lag+1, ndim) if the shape of chain is (n_samples, ndim)\n",
    "        -> (max_lag+1,) if the shape of the chain is (n_samples,).\n",
    "    \"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5a0f5cb799867aeb028cff4ff4fef0d",
     "grade": true,
     "grade_id": "P3-testautocorr",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a correlated one-dimensional chain of samples.\n",
    "chain = np.exp(-np.arange(100)/10)\n",
    "max_lag=10\n",
    "acor = autocorrelation(chain, max_lag)\n",
    "# Zero lag => max correlation\n",
    "assert np.isclose(acor[0],1.)\n",
    "assert np.abs( acor[-1]-0.75719323 ) < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03a6fcc9b41c182785e2881c4b5a4cf8",
     "grade": false,
     "grade_id": "P3c-table",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the four different proposal widths (indicated above) and extract the acceptance ratio and the correlation time(s) \n",
    "# for bivariate posterior PDF of Problem 3b. Use 10,000 samples for each sampling and a max lag of 2,000.\n",
    "# Print also the median values of the sampled parameters.\n",
    "print('Two-dimensional MCMC sampling\\n')\n",
    "print('width   acc. ratio   tau[a]   tau[b]    <a>     <b>')\n",
    "no_of_samples=10000\n",
    "\n",
    "\n",
    "for iax, width in enumerate([0.01, 0.05, 0.1, 0.5]):\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "    print(f' {width:.2f}      {acc_ratio:.2f}        {tau[0]:4d}     {tau[1]:4d}',\\\n",
    "         f'   {np.median(samples[:,0]):4.2f}   {np.median(samples[:,1]):4.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Bayesian linear regression (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fitting a linear model (in this case a first order polynomial) to a set data. Our model has two parameters $\\vec\\theta=[\\theta_0,\\theta_1]$ (pay attention to the indexing)\n",
    "\n",
    "$$\n",
    "y_M(x) = \\theta_0 + \\theta_1 x \n",
    "$$\n",
    "\n",
    "And our statistical model assumes that errors are independent and identically distributed\n",
    "\n",
    "$$\n",
    "y_i = y_M(x_i;\\theta) + \\varepsilon_i.\n",
    "$$\n",
    "\n",
    "Specifically, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ and we assume a fixed standard deviation $\\sigma = 40$. \n",
    "\n",
    "(Note that the $\\varepsilon_i \\sim \\ldots$ notation means that $\\varepsilon_i$ is a draw of a random variable that follows the specified distribution.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is generated from a straight line with intercept = 15. and slope = 1.5 plus random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 15.\n",
    "slope = 1.5\n",
    "theta_true = np.array([intercept, slope])\n",
    "sigma=40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the file `PS2_Prob4_data.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dd517c45e4d41c20b1926e35aec8596",
     "grade": false,
     "grade_id": "cell-0609945db7cac2cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data and plot with fixed error bar \n",
    "# Use np.loadtxt() for loading data (the argument 'unpack=True' is useful)\n",
    "# and plt.errorbar() for plotting data with errorbars\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Bayesian linear regression: Implement log prior(s) and likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata)** Create functions that return the natural logarithm of the likelihood and two different choices of prior. Since we are seeking a parameter posterior you do not have to use proper normalization of the probability densities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two different choices for the prior:\n",
    "1. A uniform prior:\n",
    "   $$\n",
    "   p(\\theta_0, \\theta_1 | I) \\propto \\left\\{ \\begin{array}{ll} 1 & \\text{if } -100 \\le \\theta_0 \\le 100 \\text{ and } -100 \\le \\theta_1 \\le 100 \\\\ 0 & \\text{else} \\end{array}\\right.\n",
    "   $$\n",
    "2. A uniform prior for the intercept and a symmetry-invariant one for the slope\n",
    "   $$\n",
    "   p(\\theta_0, \\theta_1 | I) \\propto \\left\\{ \\begin{array}{ll} \\frac{1}{(1+\\theta_1^2)^{3/2}} & \\text{if } -100 \\le \\theta_0 \\le 100  \\\\ 0 & \\text{else}\\end{array}\\right.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4809fccf1e4e2cea8aa848b42f8b2f34",
     "grade": false,
     "grade_id": "cell-85c54c4a189fc811",
     "locked": true,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_flat_prior(theta):\n",
    "    '''\n",
    "    Returns the log uniform prior (-100 <= theta_i <= 100)\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        \n",
    "    Returns:\n",
    "        logPi: (float) log prior (not necessarily normalized)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "def log_symmetric_prior(theta):\n",
    "    '''\n",
    "    Returns the log uniform (for theta_0) and symmetric (for theta_1) prior \n",
    "    \n",
    "    (-100 <= theta_0 <= 100; p(theta_1) \\propto (1+theta_1^2)^(-3/2))\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        \n",
    "    Returns:\n",
    "        logPi: (float) log prior (not necessarily normalized)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(log_flat_prior([0.,0.]), (np.floating, float)), 'The output should be a float'\n",
    "assert isinstance(log_symmetric_prior([0.,0.]), (np.floating, float)), 'The output should be a float'\n",
    "assert log_flat_prior([0.,0.]) - log_flat_prior([10.,-10.]) == 0., 'The flat prior should be constant in the interval'\n",
    "assert np.abs(log_symmetric_prior([-10.,1.]) - log_symmetric_prior([20.,2.]) - 1.3744360978112324) <0.001, \\\n",
    "'The log symmetric prior does not evaluate correctly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd0107dd227ac58207c31983b72d42ca",
     "grade": false,
     "grade_id": "cell-958720f73ee427a3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, dy=sigma):\n",
    "    '''\n",
    "    Returns the log likelihood.\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        x: data (independent variable). array of floats\n",
    "        y: data (dependent variable). array of floats\n",
    "        dy: fixed error (optional; default=sigma defined above), standard deviation of a normal distribution\n",
    "        \n",
    "    Returns:\n",
    "        logL: (float) log likelihood\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test = np.loadtxt(f'{DATA_DIR}/PS2_Prob4_data.txt',unpack=True)\n",
    "assert isinstance(log_likelihood([0,0], x_test, y_test), (np.floating, float)), 'The output should be a float'\n",
    "assert np.abs(log_likelihood([0.,0.], x_test, y_test) - log_likelihood([10.,1.], x_test, y_test) + 16.588440096874997) <0.001, \\\n",
    "'The log likelihood does not evaluate correctly.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Use MCMC sampling and plot the posterior for the two different prior choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks (see also Yata)**\n",
    "* Where is the mode of the posterior with these two different priors?\n",
    "* Use your sampling code, or use the `emcee` package, to collect 100,000 samples of the posterior with these two different priors.\n",
    "* Plot the joint posterior pdf for the two different prior choices using `corner`. Use the keyword argument `truths` to indicate the \"true\" values of the parameters that generate the data (in the array `theta_true` defined at the start of this problem).\n",
    "* Are the parameters correlated / anti-correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "185f8d4c6ee5e52bf3099a9cea39a367",
     "grade": false,
     "grade_id": "P4-sampleFlat",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Collect samples from the posterior with the flat prior\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbcb57cc4cf096153a27f9e299491da1",
     "grade": false,
     "grade_id": "P4-plotFlat",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make a corner plot of the posterior with the flat prior.\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a0131a8922b47fbd1e254415f73fcef",
     "grade": false,
     "grade_id": "P4-sampleSymm",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Collect samples from the posterior with the symmetric prior\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42eb134d058d150df2355d4f10e9f5d4",
     "grade": false,
     "grade_id": "P4-plotSym",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make a corner plot of the posterior with the symmetric prior.\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Unknown experimental error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the analysis but this time without knowledge of $\\sigma$, the standard deviation of the experimental errors. That means that you need to include this width as an unknown (hyper-)parameter, $\\sigma_e$, in your statistical model.\n",
    "\n",
    "- Assume a uniform prior for the width in the range [1, 100]: $\\sigma_e \\sim \\mathcal{U}(1,100)$.\n",
    "- Restrict the analysis to using only the symmetric prior for the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks (see also Yata)**\n",
    "* Use your sampling code, or use the `emcee` package, to collect 500,000(!) samples of the posterior with the symmetric prior and the unknown error. There are now three parameters.\n",
    "* Plot the joint posterior pdf using `corner`. Use the keyword argument `truths` to indicate the \"true\" values of the parameters that generate the data (the model parameters are in the array `theta_true` and the noise that was used is in the variable `sigma`).\n",
    "* Study in particular the tails of the marginal posterior distributions for the slope and the intercept. Are they different from before when the width was fixed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "530de2b4372709c5ed239dff7cf21285",
     "grade": false,
     "grade_id": "P4-sampleSymmW",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Collect samples from the posterior with the symmetric prior and the unknown width\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8f1a9b5f7fe25c80f1296d1df2f8e75",
     "grade": false,
     "grade_id": "P4-plotSymmW",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make a corner plot of the posterior with the symmetric prior and the unknown width\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
